{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "540b364a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 11:05:01.991038: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-06 11:05:02.085959: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-06 11:05:02.086016: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-06 11:05:02.088195: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-06 11:05:02.102391: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-06 11:05:03.810551: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-05-06 11:05:05.571202: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-05-06 11:05:05.571269: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: sheldon\n",
      "2024-05-06 11:05:05.571280: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: sheldon\n",
      "2024-05-06 11:05:05.571557: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 535.171.4\n",
      "2024-05-06 11:05:05.571616: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 535.161.7\n",
      "2024-05-06 11:05:05.571629: E external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:244] kernel version 535.161.7 does not match DSO version 535.171.4 -- cannot find working devices in this configuration\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# More information : https://www.tensorflow.org/tutorials/load_data/tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d099c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_SMILES = pickle.load(open(\"tokenizer_smiles.pkl\", \"rb\"))\n",
    "tokenizer_IUPAC = pickle.load(open(\"tokenizer_iupac.pkl\", \"rb\"))\n",
    "SMILES_max_length = 74\n",
    "IUPAC_max_length = 133\n",
    "Total_file_size = 102400\n",
    "path_to_file = \"Split_STOUT_IWOMI_data.txt\"\n",
    "num_chunk = 10  # Total number of training files\n",
    "file_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "189f3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w: str) -> str:\n",
    "    \"\"\"Preprocesses a sentence by converting to ASCII, adding start and end tokens, and spacing punctuation.\n",
    "\n",
    "    Args:\n",
    "        w (str): Input sentence.\n",
    "\n",
    "    Returns:\n",
    "        str: Preprocessed sentence.\n",
    "    \"\"\"\n",
    "    w = \"<start> \" + w + \" <end>\"\n",
    "    return w\n",
    "\n",
    "\n",
    "def create_dataset(line: str) -> list:\n",
    "    \"\"\"Creates a dataset from a line by preprocessing sentences.\n",
    "\n",
    "    Args:\n",
    "        line (str): Input line containing tab-separated SMILES and IUPAC names.\n",
    "\n",
    "    Returns:\n",
    "        list: List of preprocessed sentences.\n",
    "    \"\"\"\n",
    "    word_pairs = [preprocess_sentence(w) for w in line.strip().split(\"\\t\")]\n",
    "    return word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2554f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(\n",
    "    smiles: str,\n",
    "    IUPAC: str,\n",
    "    SMILES_max_length: int = SMILES_max_length,\n",
    "    IUPAC_max_length: int = IUPAC_max_length,\n",
    ") -> tuple:\n",
    "    \"\"\"Tokenizes SMILES and IUPAC names using pre-trained tokenizers.\n",
    "\n",
    "    Args:\n",
    "        smiles (str): SMILES representation.\n",
    "        IUPAC (str): IUPAC name.\n",
    "        SMILES_max_length (int): Maximum length for SMILES sequences.\n",
    "        IUPAC_max_length (int): Maximum length for IUPAC sequences.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing tokenized SMILES and IUPAC tensors.\n",
    "    \"\"\"\n",
    "    smiles_tokens = tokenizer_SMILES.texts_to_sequences([smiles])\n",
    "    iupac_tokens = tokenizer_IUPAC.texts_to_sequences([IUPAC])\n",
    "\n",
    "    smiles_tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        smiles_tokens, padding=\"post\", maxlen=SMILES_max_length\n",
    "    )\n",
    "    iupac_tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        iupac_tokens, padding=\"post\", maxlen=IUPAC_max_length\n",
    "    )\n",
    "    return smiles_tensor, iupac_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9554b177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value: bytes) -> tf.train.Feature:\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\n",
    "\n",
    "    Args:\n",
    "        value (bytes): Input byte value.\n",
    "\n",
    "    Returns:\n",
    "        tf.train.Feature: TensorFlow Feature containing the byte value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()  # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20564628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(SMILES_tensor: tf.Tensor, IUPAC_tensor: tf.Tensor) -> bytes:\n",
    "    \"\"\"Generates a TFRecord feature from SMILES and IUPAC tensors.\n",
    "\n",
    "    Args:\n",
    "        SMILES_tensor (tf.Tensor): Tokenized and padded SMILES tensor.\n",
    "        IUPAC_tensor (tf.Tensor): Tokenized and padded IUPAC tensor.\n",
    "\n",
    "    Returns:\n",
    "        bytes: Serialized TFRecord feature.\n",
    "    \"\"\"\n",
    "    feature = {\n",
    "        # 'image_id': _bytes_feature(image_id_.encode('utf8')),\n",
    "        \"input_smiles\": _bytes_feature(SMILES_tensor.tostring()),\n",
    "        \"target_iupac\": _bytes_feature(IUPAC_tensor.tostring()),\n",
    "    }\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    serialized = example.SerializeToString()\n",
    "    return serialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67c1b0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_236433/1414788491.py:13: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  \"input_smiles\": _bytes_feature(SMILES_tensor.tostring()),\n",
      "/tmp/ipykernel_236433/1414788491.py:14: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  \"target_iupac\": _bytes_feature(IUPAC_tensor.tostring()),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_data/train-Split_STOUT_IWOMI_data.txt_00.tfrecord write to tfrecord success!\n",
      "Training_data/train-Split_STOUT_IWOMI_data.txt_01.tfrecord write to tfrecord success!\n",
      "Training_data/train-Split_STOUT_IWOMI_data.txt_02.tfrecord write to tfrecord success!\n",
      "Training_data/train-Split_STOUT_IWOMI_data.txt_03.tfrecord write to tfrecord success!\n",
      "Training_data/train-Split_STOUT_IWOMI_data.txt_04.tfrecord write to tfrecord success!\n",
      "Training_data/train-Split_STOUT_IWOMI_data.txt_05.tfrecord write to tfrecord success!\n",
      "Training_data/train-Split_STOUT_IWOMI_data.txt_06.tfrecord write to tfrecord success!\n",
      "Training_data/train-Split_STOUT_IWOMI_data.txt_07.tfrecord write to tfrecord success!\n",
      "Training_data/train-Split_STOUT_IWOMI_data.txt_08.tfrecord write to tfrecord success!\n",
      "Training_data/train-Split_STOUT_IWOMI_data.txt_09.tfrecord write to tfrecord success!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"Training_data\"):\n",
    "    # If it doesn't exist, create it\n",
    "    os.makedirs(\"Training_data\")\n",
    "with open(path_to_file, \"r\") as file:\n",
    "    chunk_size = int(Total_file_size / num_chunk)  # set this proper divisible\n",
    "    processed_lines = []\n",
    "    for i, line in enumerate(file):\n",
    "        SMILES, IUPAC_names = create_dataset(line)\n",
    "        SMILES_tensor, IUPAC_tensor = tokenize(SMILES, IUPAC_names)\n",
    "        feature = get_feature(SMILES_tensor, IUPAC_tensor)\n",
    "        processed_lines.append(feature)\n",
    "        if (i + 1) % chunk_size == 0:\n",
    "            tfrecord_name = (\n",
    "                \"Training_data/\"\n",
    "                + \"train-\"\n",
    "                + path_to_file\n",
    "                + \"_%02d.tfrecord\" % file_index\n",
    "            )\n",
    "            writer = tf.io.TFRecordWriter(tfrecord_name)\n",
    "            for j in range(len(processed_lines)):\n",
    "                writer.write(processed_lines[j])\n",
    "            print(\"%s write to tfrecord success!\" % tfrecord_name)\n",
    "            file_index = file_index + 1\n",
    "            processed_lines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb0903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa95d697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DECIMER_V2",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
