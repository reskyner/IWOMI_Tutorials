{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecd9fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "import deepsearch as ds\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pdf2doi\n",
    "import csv\n",
    "import pandas as pd\n",
    "pdf2doi.config.set(\"verbose\", False)\n",
    "from get_extraction import get_spans\n",
    "from ipydatagrid import DataGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ef7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROFILE_NAME = \"\"  # the profile to use\n",
    "PROJ_KEY = (\n",
    "    \"\"  # Project Key : https://ds4sd.github.io/deepsearch-toolkit/#getting-started\n",
    ")\n",
    "api = ds.CpsApi.from_env(profile_name=PROFILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35136407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_download_documents(source_path: Path, api, proj_key, output_dir: Path):\n",
    "    \"\"\"Converts and downloads documents using Deepgram's API.\n",
    "\n",
    "    Args:\n",
    "        source_path (Path): Path to the source documents.\n",
    "        api: Deepgram API.\n",
    "        proj_key: Project key.\n",
    "        output_dir (Path): Output directory to save the converted documents.\n",
    "    \"\"\"\n",
    "    document = ds.convert_documents(\n",
    "        api=api, proj_key=proj_key, source_path=source_path, progress_bar=True\n",
    "    )\n",
    "    document.download_all(result_dir=output_dir, progress_bar=True)\n",
    "\n",
    "\n",
    "def process_zip_archive(zip_file: Path) -> Path:\n",
    "    \"\"\"Processes a zip archive, extracts JSON files, and returns the path of the extracted file.\n",
    "\n",
    "    Args:\n",
    "        zip_file (Path): Path to the zip archive.\n",
    "\n",
    "    Returns:\n",
    "        Path: Path to the extracted JSON file.\n",
    "    \"\"\"\n",
    "    with ZipFile(zip_file) as archive:\n",
    "        all_files = archive.namelist()\n",
    "        for archive_file in all_files:\n",
    "            if archive_file.endswith(\".json\"):\n",
    "                archive.extract(archive_file, zip_file.parent)\n",
    "                return zip_file.parent / archive_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bfc27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = Path(\"publication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6805870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'convert_and_download_documents(\\n        source_path=input_dir,\\n        api=api, \\n        proj_key=PROJ_KEY, \\n        output_dir=input_dir\\n)\\nprint(\"Done\")\\nfor zip_file in [file for file in input_dir.iterdir() if file.name[-4:] == \".zip\"]:\\n    process_zip_archive(zip_file=zip_file)\\n    zip_file.unlink()'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"convert_and_download_documents(\n",
    "        source_path=input_dir,\n",
    "        api=api, \n",
    "        proj_key=PROJ_KEY, \n",
    "        output_dir=input_dir\n",
    ")\n",
    "print(\"Done\")\n",
    "for zip_file in [file for file in input_dir.iterdir() if file.name[-4:] == \".zip\"]:\n",
    "    process_zip_archive(zip_file=zip_file)\n",
    "    zip_file.unlink()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec2cfc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pdf2doi.pdf2doi(\"publication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "157a25fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Talarodrides A-F, Nonadrides from the Antarctic Sponge-Derived Fungus Talaromyces sp. HDN1820200\n",
      "DOI:  10.1021/acs.jnatprod.1c00203\n",
      "19\n",
      "Text has been written to publication/Training_data/1_10.1021.acs.jnatprod.1c00203.txt\n"
     ]
    }
   ],
   "source": [
    "json_folder_path = \"publication\"\n",
    "outpu_folder_path = \"publication/Training_data/\"\n",
    "counter = 0\n",
    "for json_file_name in os.listdir(json_folder_path):\n",
    "    if json_file_name.endswith(\".json\"):\n",
    "        json_file_path = os.path.join(json_folder_path, json_file_name)\n",
    "\n",
    "        # Read JSON data from the file\n",
    "        with open(json_file_path, \"r\") as file:\n",
    "            json_data = file.read()\n",
    "\n",
    "        # Parse the JSON data\n",
    "        data = json.loads(json_data)\n",
    "        text_segments = []\n",
    "        # Extract title\n",
    "        try:\n",
    "            title = data[\"description\"][\"title\"]\n",
    "            print(\"Title:\", title)\n",
    "        except Exception as e:\n",
    "            for item in data.get(\"main-text\", []):\n",
    "                if item.get(\"type\") == \"subtitle-level-1\":\n",
    "                    extracted_text = item.get(\"text\")\n",
    "                    print(extracted_text)\n",
    "                    title = extracted_text\n",
    "                    break\n",
    "\n",
    "        text_segments.append(\"Title:\" + title + \"\\t\")\n",
    "\n",
    "        # Extract DOI from all text fields\n",
    "        doi = results[0][\"identifier\"]\n",
    "        print(\"DOI: \", doi)\n",
    "        # Extract text from abstract to Results and Discussion\n",
    "        for item in data[\"main-text\"]:\n",
    "            if \"text\" in item and \"ABSTRACT\" in item[\"text\"]:\n",
    "                start_index = data[\"main-text\"].index(item)\n",
    "                break\n",
    "\n",
    "        for item in data[\"main-text\"][start_index:]:\n",
    "            if \"text\" in item and \"RESULTS\" in item[\"text\"]:\n",
    "                end_index = data[\"main-text\"].index(item)\n",
    "                print(end_index)\n",
    "                break\n",
    "\n",
    "        for item in data[\"main-text\"][start_index:end_index]:\n",
    "            if \"text\" in item:\n",
    "                # Exclude specific patterns\n",
    "                if re.match(r\"© 2021 .*\", item[\"text\"]):\n",
    "                    continue\n",
    "                if \"* s ı\" in item[\"text\"]:\n",
    "                    continue\n",
    "                if re.match(r\"Supporting Information\", item[\"text\"]):\n",
    "                    continue\n",
    "\n",
    "                text_segments.append(item[\"text\"])\n",
    "\n",
    "        full_text = \"\\t\".join(text_segments)\n",
    "\n",
    "        # Write the extracted text to a file with the filename as the DOI\n",
    "\n",
    "        if doi:\n",
    "            text_output_file_path = os.path.join(\n",
    "                outpu_folder_path, f'{str(counter+1)+\"_\"+doi.replace(\"/\", \".\")}.txt'\n",
    "            )\n",
    "            with open(text_output_file_path, \"w\") as text_output_file:\n",
    "                text_output_file.write(full_text)\n",
    "\n",
    "            print(f\"Text has been written to {text_output_file_path}\")\n",
    "\n",
    "            # Save the original JSON file with the filename as the DOI\n",
    "            json_output_file_path = os.path.join(\n",
    "                outpu_folder_path, f'{str(counter+1)+\"_\"+doi.replace(\"/\", \".\")}.json'\n",
    "            )\n",
    "            with open(json_output_file_path, \"w\") as json_output_file:\n",
    "                json.dump(data, json_output_file, indent=4)\n",
    "            counter = counter + 1\n",
    "        else:\n",
    "            print(\"DOI not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47a0aba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'compound_group': \"Talarodrides A-F', 'talarodrides A-F(1-6)'\", 'compound_class': 'Nonadrides, dimeric maleic anhydride nonadrides, maleic anhydride nonadrides', 'organism_part': 'nan', 'organism_or_species': 'Talaromyces sp', 'geo_location': 'Antarctic', 'Kingdom': 'Fungi', 'trivial_name': 'Talarodride A (1), talarodride B (2)', 'location': 'Antarctic Sponge', 'iupac_name': 'nan', 'abbreviation': 'nan', 'iupac_like_name': 'nan', 'DOI': '10.1021/acs.jnatprod.1c00203'}\n"
     ]
    }
   ],
   "source": [
    "extracted_text, data_dict, positions = get_spans(full_text)\n",
    "data_dict['DOI'] = doi\n",
    "print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02057acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_COCONUT_input(text_data, SMILES_list):\n",
    "    df = pd.DataFrame(columns=['canonical_smiles', 'reference_id', 'name', 'doi', 'link', 'organism',\n",
    "                               'organism_part', 'coconut_id', 'mol_filename', 'structural_comments',\n",
    "                               'geo_location', 'location'])\n",
    "\n",
    "    for smiles in SMILES_list:\n",
    "        row = {'canonical_smiles': smiles,\n",
    "               'reference_id': '',\n",
    "               'name': text_data['trivial_name'].split(\",\")[0],\n",
    "               'doi': text_data['DOI'],\n",
    "               'link': '',\n",
    "               'organism': text_data['organism_or_species'],\n",
    "               'organism_part': text_data['organism_part'],\n",
    "               'coconut_id': '',\n",
    "               'mol_filename': '',\n",
    "               'structural_comments': '',\n",
    "               'geo_location': text_data['geo_location'],\n",
    "               'location': text_data['location']}\n",
    "        df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf909481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CC(=O)NC1=CC=C(C=C1)OO', 'CC(C)CC1=CC=C(C=C1)C(C)C(=O)OO', 'CC(C)CC1=CC=C(C=C1)C(C)C(=O)OO', 'CC(C)CC1=CC=C(C=C1)C(C)C(=O)OC[C@@H]2[C@@H](CO)O[C@](CO)([C@H]2O)OC3[C@@H]([C@H]([C@@H]([C@@H](COC4[C@@H]([C@H]([C@H]([C@@H](CO)O4)O)O)O)O3)O)O)OOO']\n"
     ]
    }
   ],
   "source": [
    "with open('Image_Data_Extraction/final_output.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    smiles_list = []\n",
    "    for row in reader:\n",
    "        smiles = row['Predicted Smiles']\n",
    "        smiles_list.append(smiles)\n",
    "\n",
    "print(smiles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fb99c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_COCONUT_input(data_dict,smiles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9df98748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e8d40fbe334be586fc295895c91184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DataGrid(auto_fit_params={'area': 'all', 'padding': 30, 'numCols': None}, base_column_size=300, base_row_size=…"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = DataGrid(df, base_row_size=30, base_column_size=300, editable=True)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b388b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = grid.data\n",
    "updated_df.to_csv('data_updated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe89dd75-4b70-4ccd-aaa0-a5da94e5689d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
